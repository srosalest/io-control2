{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5rfZ_hYaQ1Z"
   },
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"30%\" />\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2020 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 3 - Modelos Avanzados </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Formalidades**  \n",
    "* Equipos de trabajo de: 3 personas (*cada uno debe estar en condiciones de realizar una presentación y discutir sobre cada punto del trabajo realizado*)\n",
    "* Formato de entrega: envı́o de link Github y link de video Youtube o plataforma a convenir, todo esto vía Aula. \n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "### **Propuesta**\n",
    "* Se debe preparar una presentación de **15 a 20 minutos** donde se explique el cómo se va a realizar/resolver el taller, la metodología o propuesta de las componentes a experimentar y explorar. Más detalles en el Syllabus.\n",
    "* Fecha de encuentro Zoom: 24 de Julio en horario de clases.\n",
    "* Fecha de entrega de vídeo: Opcional para quienes presentaron y obligatorio para quienes no, a lo más 2 días después del encuentro.\n",
    "* Modalidad de Presentación (Zoom): En el primer bloque, se formarán 3 grupos para que alcancen a recibir feedback todos los equipos. En el segundo bloque, algunos equipos seleccionados presentarán a todo el curso. \n",
    "\n",
    "**Aún si la idea es aprender colaborativamente, valoraremos mucho la diversidad de ideas, por lo que las propuesta debiesen conservar su orientación inicial, excepto por el feedback que les entreguemos**\n",
    "\n",
    "### **Defensa**\n",
    "* Se debe preparar una presentación de **15 a 20 minutos** con los resultados obtenidos y conclusiones de la experiencia. \n",
    "* Se debe entregar el código, de preferencia en un (breve) Jupyter/IPython notebook, de modo que **permita reproducir los resultados** presentados. Si se entrega el código fuente se deben proveer instrucciones para su uso.\n",
    "* Fecha de encuentro Zoom: 7 de Agosto, horario de clases.\n",
    "* Fecha de entrega de vídeo y Jypter (notebook): 7 de Agosto.\n",
    "* Modalidad de Presentación (Zoom): En ambos bloques algunos equipos seleccionados presentarán ante todo el curso, discusión y debate se generará en base a los resultados.\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) Desafío por Método  \n",
    "[2.](#segundo) Desafío por Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LuudGKMiAj3Z"
   },
   "source": [
    "#### <a id=\"primero\"></a>\n",
    "## 1. Desafío por Método: Neural Information Retrieval\n",
    "\n",
    "<img src=\"https://sites.google.com/site/xinchaoliweb/_/rsrc/1453028120793/research/ObjectRetrieval.jpg\" title=\"Title text\" width=\"80%\" />\n",
    "\n",
    "La tarea de un *sistema de recuperación de información (IR)* consiste en entregar los elementos de una cierta base de datos que resultan más relevantes para una determinada \"consulta\" (query) al sistema. Esta \"consulta\" se entrega al sistema en un formato pre-acordado con el usuario: por ejemplo podría tratarse de la conjunción de ciertas palabras claves.\n",
    "\n",
    "**Recuperación por Contenido:** En muchas aplicaciones, la base de datos está formada por objetos poco estructurados y ricos en contenido como una imagen, un video o una canción, e interesa poder recuperar los objetos más \"similares\" a una determinada query en términos semánticos. En este caso, más que términos claves, es práctico que la query pueda ser del mismo tipo de los **objetos** indexados: una imagen para recuperar imágenes o una canción para recuperar canciones. En este caso, el IR debe aprender qué objetos son los relevantes para una query porque la búsqueda se hace ya no guiada por términos claves o por instrucciones muy estructuradas, sino por contenido. Esto es aplicado a diferentes dominios: texto, imágenes, audio, vídeos. \n",
    "\n",
    "**Recuperación Inter-dominio:** En el problema de recuperación de información inter-dominio (**cross-modal** ó **cross-domain**), el objetivo es recuperar los objetos similares a un objeto de otro dominio. Por ejemplo, desde un texto que describe una situación, recuperar las imágenes similares (*Text2Image*), o desde una imagen recuperar los textos más similares a la situación en la imagen (*Image2Text*). Este último problema (Image2Text) puede ser similar a lo que se hace en Image Captioning pero el objetivo no es generar un texto nuevo, sino que se deben encontrar en un conjunto bajo una cierta similaridad.\n",
    "\n",
    "<img src=\"http://www.svcl.ucsd.edu/projects/crossmodal/images/problem.png\" title=\"Title text\" width=\"50%\" />\n",
    "\n",
    "Su tarea será utilizar redes neuronales para construir un IR basado en contenido del tipo\n",
    "1. mono-modal *image-2-image*  \n",
    "2. bi-modal *text-2-image*  \n",
    "3. bi-modal *image-2-text*  \n",
    "> **Importante**: Deberá escoger y resolver 2 de los 3 problemas \n",
    "\n",
    "Para testear y validar su solución tendrá a disposición diferentes datasets.\n",
    "Para el problema 1 puede utilizar cualquier dataset de imágenes RGB etiquetados que conozca (por ejemplo CIFAR) y construir los pares de imágenes en base a las categorías del problema: si comparten la misma clase se puede crear el par (imagen1-clase-K,imagen2-clase-K). Para el problema 2 y 3 utilice el dataset **Flickr8k**, una colección medianamente grande de imágenes que contiene 3 breves descripciones por cada una. Para entrenar puede utilizar sólo los datos de **Flickr8k** o algún otro dataset, con pares del tipo (imagen,texto), para complementar su propuesta.\n",
    "\n",
    "> Flickr8k: https://github.com/jbrownlee/Datasets/releases/tag/Flickr8k  \n",
    "> También podría utilizar algún otro como MSCOCO: https://cocodataset.org/#captions-2015  \n",
    "\n",
    "Recuerde que en fase de test, el sistema debe recibir una consulta y devolver una lista de resultados, que podemos asumir serán una serie de identificadores sobre la base de datos, por ejemplo: 2030, 3, 10, 120, 617.\n",
    "\n",
    "Su presentación de la solución debe incluir una clara descripción de (1) cómo se representan las consultas/objetos para ingresar al sistema, (2) cómo \n",
    "se entrena el modelo, (3) cómo evalúa el resultado final.\n",
    "\n",
    "* **Bonus** Se asigná un bonus de hasta 20 puntos sobre toda la tarea, a quienes exploren eventuales **sesgos de género** en el modelo, es decir, si la recuperación de los objetos evidencia diferencias significativas entre imágenes masculinas o femeninas (esas están anotadas). Por ejemplo, cuando ante la palabra \"conducir\" aperecen muchas más imágenes con personas clasificadas como masculino ó bien cuando la palabra \"woman\" o \"man\" se encuentra en los textos, se recuperan imágenes con un cierto *sesgo* (*bias*) visual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nDWquLhAj7q"
   },
   "source": [
    "#### <a id=\"segundo\"></a>\n",
    "## 2. Desafío por Score: Modelo Multi-tareas en Audio\n",
    "\n",
    "En esta sección se trabajará en el dominio de audio, como solicitaron. En particular se contará con audios de llantos de bebés, los cuales habrá que clasificar en base a diferentes categorías. Para hacer el problema mucho más desafiante el objetivo será **multi-task**, es decir, predecir múltiples tareas y objetivos simultáneamente (nos interesa que aprendan a construir una arquitectura para este tipo de problemas).\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/9l3tjzgyn9gr/photo-61067/c52112b6d064a0a5479bc520c89d5865/61067-babycrt.jpg?fm=jpg&fl=progressive&q=50&w=1200\" title=\"Title text\" width=\"30%\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eNo1Qc4X61FY"
   },
   "source": [
    "* Link Kaggle: Pendiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwPYaHsK5otT"
   },
   "source": [
    "El dataset mencionado fue recolectado con la ayuda de padres que mandaron el audio de sus bebés etiquetando ellos mismo la causa. Se contará con (i) predicción de la \"causa'' del llanto, (ii) rango etario y (iii) sexo. La métrica de evaluación tendrá en cuenta las 3 tareas.\n",
    "\n",
    "* Causas del llanto: *hungry, needs burping, belly pain, discomfort, tired, lonely,cold/hot, scared,don't know*\n",
    "\n",
    "El dataset disponible es pequeño, desbalanceado y, como decíamos, corresponde a audio. Hay muchos modos de representar el audio. Por ejemplo, si toma la Transformada de Fourier Reducida (Short-time Fourier Transform, STFT) de los audios o extrae Mel Frequency Cepstral Coefficients (MFCC), obtendrá imágenes, pudiendo aplicar todo lo que ha aprendido de ese dominio. El desbalanceo y el pequeño tamaño del dataset se puede manejar haciendo **data augmentation** o de otro modo que usted proponga. Finalmente, para manejar la escacez de datos de entrenamiento podría utilizar alguna de estas ideas: (i) red pre-entrenada (ya sabemos que funciona), (ii) expansión del dataset con otro dataset que usted consiga de otra parte \n",
    "\n",
    "1.   Por ejemplo acá hay audios de bebés riendo y bebés llorando: https://research.google.com/audioset/dataset/baby_laughter.html\n",
    "2.   Acá hay audios de bebés y otros animales: https://github.com/karolpiczak/ESC-50\n",
    "3. Acá hay audios de llantos de bebés, vidrio quebrado y disparos: http://www.cs.tut.fi/sgn/arg/dcase2017/challenge/task-rare-sound-event-detection#audio-dataset\n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[ANN]Taller3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
